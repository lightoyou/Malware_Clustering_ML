#-*- coding: utf-8 -*-
import lief
from lief import to_json
import hashlib
import json
from collections import Counter
import os
#import machocke

'''
def string_extractor():
    # 0x20-0x7f avec +5 caractères
    allstrings = re.compile(b'[\x20-\x7f]{5,}')
    # occureances de la chaîne "C:\"
    paths = re.compile(b'c:\\\\', re.IGNORECASE)
    # occureances de la chaîne "http://" ou "https://"
    urls = re.compile(b'https?://', re.IGNORECASE)
    # occureances de la chaîne "HKEY_" (registre windows)
    registry = re.compile(b'HKEY_')
    mz = re.compile(b'MZ')


    stringsDict{
            'numstrings': len(allstrings),
            'avlength': avlength,
            'printabledist': c.tolist(),
            'printables': int(csum),
            'entropy': float(H),
            'paths': len(paths.findall(bytez)),
            'urls': len(urls.findall(bytez)),
            'registry': len(registry.findall(bytez)),
            'MZ': len(mz.findall(bytez))
        }

'''


def get_hashes(data,isFile):
    if isFile == True:
        afile = open(data, 'rb')
        buf = afile.read()
    elif isFile == False:
        buf = data

    md5_hash = hashlib.md5()
    sha1_hash = hashlib.sha1()
    sha256_hash = hashlib.sha256()
    #ssdeep_hash = ssdeep.Hash()

    md5_hash.update(buf)
    sha1_hash.update(buf)
    sha256_hash.update(buf)
    #Todo machocke
    #ssdeep_hash.update(bytes(data))
    #return #ssdeep_hash.digest()
    return md5_hash.hexdigest(), sha1_hash.hexdigest(), sha256_hash.hexdigest()


def get_general_infos(lief_binary):
    generalInfoDict ={
            #"size": len(bytez),
            "vsize": lief_binary.virtual_size,
            "has_debug": int(lief_binary.has_debug),
            "exports": len(lief_binary.exported_functions),
            "imports": len(lief_binary.imported_functions),
            "has_relocations": int(lief_binary.has_relocations),
            "has_resources": int(lief_binary.has_resources),
            "has_signature": int(lief_binary.has_signature),
            "has_tls": int(lief_binary.has_tls),
            "symbols": len(lief_binary.symbols),
            "entrypoint" : hex(lief_binary.entrypoint),
            }
    return generalInfoDict


def get_header_infos(lief_binary):
    raw_obj = {}
    raw_obj['coff'] = {'timestamp': 0, 'machine': "", 'characteristics': []}
    raw_obj['optional'] ={
        'subsystem': "",
        'dll_characteristics': [],
        'magic': "",
        'major_image_version': 0,
        'minor_image_version': 0,
        'major_linker_version': 0,
        'minor_linker_version': 0,
        'major_operating_system_version': 0,
        'minor_operating_system_version': 0,
        'major_subsystem_version': 0,
        'minor_subsystem_version': 0,
        'sizeof_code': 0,
        'sizeof_headers': 0,
        'sizeof_heap_commit': 0
    }

    raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps
    raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]
    raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]
    raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]
    raw_obj['optional']['dll_characteristics'] = [str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists]
    raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]
    raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version
    raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version
    raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version
    raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version
    raw_obj['optional']['major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version
    raw_obj['optional']['minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version
    raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version
    raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version
    raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code
    raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers
    raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit

    return raw_obj


def dump_sections(lief_binary):
    sections = []

    for section in lief_binary.sections:
        sectionDict = {
        "name": section.name,
        "characteristics": section.characteristics,
        "vsize": hex(section.virtual_size),
        "size": hex(section.size),
        "vaddres": hex(section.virtual_address),
        "entrop": section.entropy
        }
        sections.append(sectionDict)
    return sections
        #TODOO CALC HASH OF SECTIONS
        #section.content
        #md5_sec, sha1_sec, sha256_sec = get_hashes(section.content, False)

def load(file):
    pe = lief.parse(file)
    dictPE = {
    "type" : "PE",
    "path" : file,
    "name" : pe.name,
    "appeared":"2017-01",
    # if label = 0 pas malware if label = 1 malware if label = -1 unlabel
        #if label = 1 specify type of malware ( VT REPORT)
    "label" : "-1",
    #"hashes": {},
    #"nb_sections" : len(pe.sections),
    #"imported_fonction": pe.imported_functions,
    #"imported_libraries": pe.libraries,
    #"general": {},
    #"header": {},
    #"sections": [],
    }

    dictPE["hashes"] = {}
    dictPE["hashes"]["md5"], dictPE["hashes"]["sha1"], dictPE["hashes"]["sha256"] = get_hashes(file,True)
    dictPE["general"] = {}
    dictPE["general"] = get_general_infos(pe)
    dictPE["header"] = {}
    dictPE["header"] = get_header_infos(pe)

    #sections = dump_sections(pe)
    #dictPE["sections"].append(sections)
    #print(pe.exported_functions)
    return dictPE



AllFiles = []
#PathDataset = "/home/light/Documents/Cours_CDSI/ML/dataset/theZoo/malwares/Binaries"
PathDataset = "dataset/"
for root, subFolders, files in os.walk(PathDataset):
    for file in files:
        PathDataset = os.path.join(root, file)
        rootPath = os.path.join(root)
        AllFiles.append(PathDataset)



AllPE = []

for p in AllFiles:
    print(p)
    try:
        file_json = load(p)

        AllPE.append(file_json)
    except:
        pass

#on sauvegarde les rapports json
for p in AllPE:
    json.dump( p ,open('%s.json' %os.path.join('jsonfiles', p['hashes']['sha256']),'w'))

#Check collisions not work well for now
AllSha256 = [ p['hashes']['sha256']for p in AllPE]
c = Counter()
for s in AllSha256:
    c[s] +=1
collision = [ k for k,v in c.items() if v > 1]
[(p['hashes']['sha256'] ,p['path']) for p in AllPE if p['hashes']['sha256'] in collision]
